#! /usr/bin/env python2
# -*- encoding: utf-8; py-indent-offset: 4 -*-
#
# Author:  Linuxfabrik GmbH, Zurich, Switzerland
# Contact: info (at) linuxfabrik (dot) ch
#          https://www.linuxfabrik.ch/
# License: The Unlicense, see LICENSE file.

# https://git.linuxfabrik.ch/linuxfabrik-icinga-plugins/checks-linux/-/blob/master/CONTRIBUTING.md

__author__  = 'Linuxfabrik GmbH, Zurich/Switzerland'
__version__ = '2020032801'

DESCRIPTION = 'Checks disk IO.'

DEFAULT_COUNT = 5       # measurements; if check runs once per minute, this is a 5 minute interval

DEFAULT_WARN = 60       # MB/s
DEFAULT_CRIT = 100      # MB/s

DEFAULT_IGNORE = ['sr0']


#====================
from lib.globals import *

import lib.base
import lib.db
from lib.output import unpack_perfdata, bytes2human

import argparse
from traceback import print_exc
try:
    import psutil
except ImportError, e:
    print('Python module "psutil" is not installed.')
    exit(STATE_UNKNOWN)


def define_args():
    parser = argparse.ArgumentParser(description=DESCRIPTION)

    parser.add_argument('-V', '--version',
        action = 'version',
        version = '{0}: v{1} by {2}'.format('%(prog)s', __version__, __author__)
        )

    parser.add_argument('--count',
        help = 'Number of times the value has to be above the given thresholds. Default: %(default)s',
        dest = 'COUNT',
        type = int,
        default = DEFAULT_COUNT,
        )

    parser.add_argument('-c', '--critical',
        help = 'The critical threshold in MB/sec for the read and write rate. Default: %(default)s',
        dest = 'CRIT',
        type = int,
        default = DEFAULT_CRIT,
        )

    parser.add_argument('--ignore',
        help = 'Ignore some disks like "sr0" or "dm-1 (repeat this parameter to add multiple disks). Default: %(default)s',
        dest = 'IGNORE',
        default = DEFAULT_IGNORE,
        action = 'append',
        )

    parser.add_argument('-w', '--warning',
        help = 'The warning threshold in MB/sec for the read and write rate. Default: %(default)s',
        dest = 'WARN',
        type = int,
        default = DEFAULT_WARN,
        )

    return parser.parse_args()


def main():    
    # parse the command line, exit with UNKNOWN if it fails
    try:
        parsed = define_args()
    except SystemExit as e:
        exit(STATE_UNKNOWN)


    # create the db table
    definition = '''
            name                TEXT NOT NULL,
            busy_time           INT NOT NULL, 
            read_bytes          INT NOT NULL,
            read_count          INT NOT NULL,
            read_merged_count   INT NOT NULL,
            read_time           INT NOT NULL,
            write_bytes         INT NOT NULL,
            write_count         INT NOT NULL,
            write_merged_count  INT NOT NULL,
            write_time          INT NOT NULL,
            timestamp           INT NOT NULL
        '''
    conn = lib.base.continue_or_exit(lib.db.connect(filename='disk-io.db'))
    lib.base.continue_or_exit(lib.db.create_table(conn, definition, drop_table_first=False))
    lib.base.continue_or_exit(lib.db.create_index(conn, 'name'))


    # get disk data and store it to database
    try:
        disk_io_counters = psutil.disk_io_counters(perdisk=True)
    except:
        print('psutil raised an error.')
        exit(STATE_UNKNOWN)

    timestamp = lib.base.now()

    disks = []
    for disk, values in disk_io_counters.items():
        if disk in parsed.IGNORE:
            continue
        data = {}
        data['name'] = disk
        data['busy_time'] = values.busy_time
        data['read_bytes'] = values.read_bytes
        data['read_count'] = values.read_count
        data['read_merged_count'] = values.read_merged_count
        data['read_time'] = values.read_time
        data['write_bytes'] = values.write_bytes
        data['write_count'] = values.write_count
        data['write_merged_count'] = values.write_merged_count
        data['write_time'] = values.write_time
        data['timestamp'] = timestamp
        disks.append(disk)
        lib.base.continue_or_exit(lib.db.insert(conn, data))

    lib.base.continue_or_exit(lib.db.cut(conn, max=parsed.COUNT*len(disks)))
    lib.base.continue_or_exit(lib.db.commit(conn))


    msg_header, msg_body, msg_item, msg_warn = '', '', '', ''
    perfdata = ''
    state = STATE_OK


    max_rw = 0       # disk with the highest sum of r/w: show this on top later on
    for disk in sorted(disks):
        # get the two newest historical data rows for a specific disk, newest item first
        diskdata = lib.base.continue_or_exit(lib.db.select(conn, 
            'SELECT * FROM perfdata WHERE name = :name ORDER BY timestamp DESC LIMIT 2',
            {'name': disk}
        ))
        if len(diskdata) != 2:
            print('Waiting for more data.')
            lib.db.close(conn)
            exit(state)

        # calculate
        timestamp_diff = diskdata[0]['timestamp'] - diskdata[1]['timestamp']            # in seconds
        read_bytes_per_second = int((diskdata[0]['read_bytes'] - diskdata[1]['read_bytes']) / timestamp_diff)
        read_count_per_second = int((diskdata[0]['read_count'] - diskdata[1]['read_count']) / timestamp_diff)
        write_bytes_per_second = int((diskdata[0]['write_bytes'] - diskdata[1]['write_bytes']) / timestamp_diff)
        write_count_per_second = int((diskdata[0]['write_count'] - diskdata[1]['write_count']) / timestamp_diff)

        perfdata += unpack_perfdata('{}_read_bytes_per_second'.format(disk), read_bytes_per_second, 'B', None, None, 0, None)
        perfdata += unpack_perfdata('{}_read_count_per_second'.format(disk), read_count_per_second, None, None, None, 0, None)
        perfdata += unpack_perfdata('{}_write_bytes_per_second'.format(disk), write_bytes_per_second, 'B', None, None, 0, None)
        perfdata += unpack_perfdata('{}_write_count_per_second'.format(disk), write_count_per_second, None, None, None, 0, None)

        msg_item = '{}: {} R/s, {} W/s, {} IO/s\n'.format(disk, bytes2human(read_bytes_per_second), bytes2human(write_bytes_per_second), read_count_per_second+write_count_per_second)
        if read_bytes_per_second + write_bytes_per_second > max_rw:
            msg_header = msg_item
            max_rw = read_bytes_per_second + write_bytes_per_second
        msg_body += '* ' + msg_item


    parsed.WARN = parsed.WARN * 1024 * 1024     # Megabytes > Bytes
    parsed.CRIT = parsed.CRIT * 1024 * 1024     # Megabytes > Bytes

    # the real check goes here - we warn about a "count" period/amount of time, not about the current situation above (what might be a peak only)
    for disk in sorted(disks):
        # get all historical data rows for a specific disk, newest item first
        diskdata = lib.base.continue_or_exit(lib.db.select(conn, 
            'SELECT * FROM perfdata WHERE name = :name ORDER BY timestamp DESC',
            {'name': disk}
        ))
        if len(diskdata) != parsed.COUNT:
            # not enough data yet
            continue

        # calculate
        timestamp_diff = diskdata[0]['timestamp'] - diskdata[parsed.COUNT - 1]['timestamp']            # in seconds
        read_bytes_per_second = int((diskdata[0]['read_bytes'] - diskdata[parsed.COUNT - 1]['read_bytes']) / timestamp_diff)
        write_bytes_per_second = int((diskdata[0]['write_bytes'] - diskdata[parsed.COUNT - 1]['write_bytes']) / timestamp_diff)

        throughput = read_bytes_per_second + write_bytes_per_second
        if throughput >= parsed.CRIT:
            msg_warn += '{} (CRIT): {}/s, '.format(disk, bytes2human(throughput))
            state = STATE_CRIT
        elif throughput >= parsed.WARN:
            msg_warn += '{} (WARN): {}/s, '.format(disk, bytes2human(throughput))
            state = STATE_WARN

    lib.db.close(conn)


    if msg_warn:
        msg_header = msg_warn[:-2] + '\n'
    msg = msg_header + msg_body

    print(msg.strip() + '|' + perfdata.strip())
    exit(state)


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print_exc()
        exit(STATE_UNKNOWN)
