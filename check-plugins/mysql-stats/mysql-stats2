#! /usr/bin/env python2
# -*- coding: utf-8; py-indent-offset: 4 -*-
#
# Author:  Linuxfabrik GmbH, Zurich, Switzerland
# Contact: info (at) linuxfabrik (dot) ch
#          https://www.linuxfabrik.ch/
# License: The Unlicense, see LICENSE file.

# https://git.linuxfabrik.ch/linuxfabrik-icinga-plugins/checks-linux/-/blob/master/CONTRIBUTING.md

import os

activate_this = False
venv_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), 'monitoring-plugins-venv2')
if os.path.exists(venv_path):
    activate_this = os.path.join(venv_path, 'bin/activate_this.py')

if os.getenv('MONITORING_PLUGINS_VENV2'):
    activate_this = os.path.join(os.getenv('MONITORING_PLUGINS_VENV2') + 'bin/activate_this.py')

if activate_this and os.path.isfile(activate_this):
    exec(open(activate_this).read(), {'__file__': activate_this})


__author__ = 'Linuxfabrik GmbH, Zurich/Switzerland'
__version__ = '2021052401'

DESCRIPTION = 'This check tests connections to a mysql server and gets some statistics (based on MySQL-Tuner).'

DEFAULT_HOSTNAME = '127.0.0.1'
DEFAULT_PORT = '3306'


#====================
from lib.globals2 import *

import lib.base2

import argparse
try:
    import mysql.connector
except ImportError as e:
    print('Python module "mysql.connector" is not installed.')
    exit(STATE_UNKNOWN)
try:
    import psutil
except ImportError as e:
    print('Python module "psutil" is not installed.')
    exit(STATE_UNKNOWN)
from traceback import print_exc


def parse_args():
    parser = argparse.ArgumentParser(description=DESCRIPTION)

    parser.add_argument('-V', '--version',
        action='version',
        version='{0}: v{1} by {2}'.format('%(prog)s', __version__, __author__)
        )

    parser.add_argument('--always-ok',
        help='Always returns OK.',
        dest='ALWAYS_OK',
        action='store_true',
        default=False,
        )

    parser.add_argument('-H', '--hostname',
        help='MySQL/MariaDB hostname. Default: %(default)s',
        dest='HOSTNAME',
        default=DEFAULT_HOSTNAME,
        )

    parser.add_argument('-p', '--password',
        help='Use the indicated password to authenticate the connection. IMPORTANT: THIS FORM OF AUTHENTICATION IS NOT SECURE.',
        dest='PASSWORD',
        required = True,
        )

    parser.add_argument('--port',
        help='MySQL/MariaDB port. Default: %(default)s',
        dest='PORT',
        default=DEFAULT_PORT,
        )

    parser.add_argument('-u', '--username',
        help='MySQL/MariaDB username.',
        dest='USERNAME',
        required = True,
        )

    return parser.parse_args()


def get_pf_memory(myvar, perfschema):
    # Performance Schema
    if 'performance_schema' not in myvar or myvar['performance_schema'] == 'OFF':
        return 0
    return perfschema['performance_schema.memory']


def get_other_process_memory():
    return 0


def get_log_file_real_path(file, hostname, datadir):
    if file and os.path.isfile(file):
        return file
    if os.path.isfile(hostname + '.log'):
        return hostname + '.log'
    if os.path.isfile(hostname + '.err'):
        return hostname + '.err'
    if os.path.isfile(datadir + '/' + hostname + '.err'):
        return datadir + '/' + hostname + '.err'
    if os.path.isfile(datadir + '/' + hostname + '.log'):
        return datadir + '/' + hostname + '.log'
    if os.path.isfile(datadir + '/mysql_error.log'):
        return datadir + 'mysql_error.log'
    if os.path.isfile('/var/log/mysql.log'):
        return '/var/log/mysql.log'
    if os.path.isfile('/var/log/mysqld.log'):
        return '/var/log/mysqld.log'
    if os.path.isfile('/var/log/mysql/' + hostname + '.err'):
        return '/var/log/mysql/' + hostname + '.err'
    if os.path.isfile('/var/log/mysql/' + hostname + '.log'):
        return '/var/log/mysql/' + hostname + '.log'
    if os.path.isfile('/var/log/mysql/mysql_error.log'):
        return '/var/log/mysql/mysql_error.log'
    return file


def log_file_recommendations(myvar):
    log_error = get_log_file_real_path(myvar['log_error'], myvar['hostname'], myvar['datadir']);
    if not log_error:
        return '* No log file set (set log_error)\n'
    if not os.path.isfile(log_error):
        return '* Log file {} doesn\'t exist\n'.format(log_error)
    if os.path.getsize(log_error) == 0:
        return '* Log file {} is empty\n'.format(log_error)
    if os.path.getsize(log_error) >= 32 * 1024 * 1024:
        return '* Log file {} is bigger than 32M (you should analyze why or implement a rotation log strategy such as logrotate)\n'.format(log_error)
    return ''


def main():
    # parse the command line, exit with UNKNOWN if it fails
    try:
        args = parse_args()
    except SystemExit as e:
        exit(STATE_UNKNOWN)

    try:
        if mysql.connector.__version__ >= '2.1.1':
            cnx = mysql.connector.connect(
                host = args.HOSTNAME,
                port = args.PORT,
                user = args.USERNAME,
                password = args.PASSWORD,
                use_pure = False,
                )
        else:
            cnx = mysql.connector.connect(
                host = args.HOSTNAME,
                port = args.PORT,
                user = args.USERNAME,
                password = args.PASSWORD,
                )
    except mysql.connector.Error as err:
        if err.errno == mysql.connector.errorcode.ER_ACCESS_DENIED_ERROR:
            print('Wrong username or password')
            exit(STATE_UNKNOWN)
        elif err.errno == mysql.connector.errorcode.ER_BAD_DB_ERROR:
            print('Unknown database')
            exit(STATE_UNKNOWN)
        else:
            print(err)
            exit(STATE_UNKNOWN)

    myvar = {}
    mystat = {}
    perfschema = {}
    mycalc = {}

    cursor = cnx.cursor()

    query = 'select version();'
    cursor.execute(query)
    version = cursor.fetchone()
    if len(version) <1:
        print('You probably did not get enough privileges for running this check.')
        exit(STATE_UNKNOWN)

    version = version[0].split('.')
    mysqlvermajor = int(version[0])
    mysqlverminor = int(version[1])
    mysqlvermicro = version[2]
    mysqltype='MariaDB' if 'MariaDB' in mysqlvermicro else 'MySQL'


    # https://dev.mysql.com/doc/refman/8.0/en/show-variables.html
    query = 'show variables;'
    cursor.execute(query)
    for (key, value) in cursor:
        myvar[key] = lib.base2.smartcast(value)
    query = 'show global variables;'
    cursor.execute(query)
    for (key, value) in cursor:
        myvar[key] = lib.base2.smartcast(value)

    # https://dev.mysql.com/doc/refman/8.0/en/show-status.html
    query = 'show status;'
    cursor.execute(query)
    for (key, value) in cursor:
        mystat[key] = lib.base2.smartcast(value)
    query = 'show global status;'
    cursor.execute(query)
    for (key, value) in cursor:
        mystat[key] = lib.base2.smartcast(value)

    query = 'select count(*) from information_schema.tables;'
    cursor.execute(query)
    nbtables = cursor.fetchone()

    query = 'show engine performance_schema status;'
    try:
        cursor.execute(query)
        for (ptype, pname, pstatus) in cursor:
            perfschema[pname] = lib.base2.smartcast(pstatus)
    except mysql.connector.Error as err:
        print('Failed querying performance_schema status. {}'.format(err))
        exit(STATE_UNKNOWN)

    cursor.close()
    cnx.close()

    if len(mystat) <1:
        print('Your server has not answered any queries - cannot continue.')
        exit(STATE_UNKNOWN)


    # calculations as done in mysqltuner.pl
    # https://github.com/major/MySQLTuner-perl/blob/master/mysqltuner.pl
    mem = psutil.virtual_memory()
    physical_memory = mem.total

    mycalc['pct_reads']  = 0
    mycalc['pct_writes']  = 0
    if mystat['Questions'] > 0:
        mycalc['total_reads'] = mystat['Com_select']
        mycalc['total_writes'] = mystat['Com_delete'] + mystat['Com_insert'] + mystat['Com_update'] + mystat['Com_replace']
        if mycalc['total_reads'] == 0:
            mycalc['pct_writes'] = 100
        else:
            mycalc['pct_reads'] = int(mycalc['total_reads'] / (mycalc['total_reads'] + mycalc['total_writes']) * 100)
            mycalc['pct_writes'] = 100 - mycalc['pct_reads'];

    # Per-thread memory
    if mysqlvermajor >= 4:
        mycalc['per_thread_buffers'] = myvar['read_buffer_size'] + myvar['read_rnd_buffer_size'] + myvar['sort_buffer_size'] + myvar['thread_stack'] + myvar['join_buffer_size']
    else:
        mycalc['per_thread_buffers'] = myvar['record_buffer'] + myvar['record_rnd_buffer'] + myvar['sort_buffer'] + myvar['thread_stack'] + myvar['join_buffer_size']
    mycalc['total_per_thread_buffers'] = mycalc['per_thread_buffers'] * myvar['max_connections']
    mycalc['max_total_per_thread_buffers'] = mycalc['per_thread_buffers'] * mystat['Max_used_connections']

    # Server-wide memory
    mycalc['max_tmp_table_size'] = myvar['max_heap_table_size'] if myvar['tmp_table_size'] > myvar['max_heap_table_size'] else myvar['tmp_table_size']
    mycalc['server_buffers'] = myvar['key_buffer_size'] + mycalc['max_tmp_table_size']
    mycalc['server_buffers'] += myvar['innodb_buffer_pool_size'] if 'innodb_buffer_pool_size' in myvar else 0
    mycalc['server_buffers'] += myvar['innodb_additional_mem_pool_size'] if 'innodb_additional_mem_pool_size' in myvar else 0
    mycalc['server_buffers'] += myvar['innodb_log_buffer_size'] if 'innodb_log_buffer_size' in myvar else 0
    mycalc['server_buffers'] += myvar['query_cache_size'] if 'query_cache_size' in myvar else 0
    mycalc['server_buffers'] += myvar['aria_pagecache_buffer_size'] if 'aria_pagecache_buffer_size' in myvar else 0

    # Global memory
    # Max used memory is memory used by MySQL based on Max_used_connections
    # This is the max memory used theoretically calculated with the max concurrent connection number reached by mysql
    mycalc['max_used_memory'] = mycalc['server_buffers'] + mycalc['max_total_per_thread_buffers'] + get_pf_memory(myvar, perfschema)

    mycalc['pct_max_used_memory'] = int(mycalc['max_used_memory'] / physical_memory * 100)

    # Total possible memory is memory needed by MySQL based on max_connections
    # This is the max memory MySQL can theoretically used if all connections allowed has opened by mysql
    mycalc['max_peak_memory'] = mycalc['server_buffers'] + mycalc['total_per_thread_buffers'] + get_pf_memory(myvar, perfschema)

    # +  get_gcache_memory();
    mycalc['pct_max_physical_memory'] = int(mycalc['max_peak_memory'] / physical_memory * 100)

    # Slow queries
    mycalc['pct_slow_queries'] = int(mystat['Slow_queries'] / mystat['Questions'] * 100)

    # Connections
    mycalc['pct_connections_used'] = int(mystat['Max_used_connections'] / myvar['max_connections'] * 100 )
    mycalc['pct_connections_used'] = 100 if mycalc['pct_connections_used'] > 100 else mycalc['pct_connections_used']

    # Aborted Connections
    mycalc['pct_connections_aborted'] = int(mystat['Aborted_connects'] / mystat['Connections'] * 100) if mystat['Connections'] else 0

    # Query cache
    if mysqltype == 'MySQL' and (mysqlvermajor >= 8 and mysqlvermajor <= 10):
        mycalc['query_cache_efficiency'] = 0
    elif mysqlvermajor >= 4:
        mycalc['query_cache_efficiency'] = mystat['Qcache_hits'] / (mystat['Com_select'] + mystat['Qcache_hits']) * 100
        if myvar['query_cache_size']:
            mycalc['pct_query_cache_used'] = 100 - (mystat['Qcache_free_memory'] / myvar['query_cache_size']) * 100
        if mystat['Qcache_lowmem_prunes'] == 0:
            mycalc['query_cache_prunes_per_day'] = 0
        else:
            mycalc['query_cache_prunes_per_day'] = int(mystat['Qcache_lowmem_prunes'] / ( mystat['Uptime'] / 86400 ))

    # Sorting
    mycalc['total_sorts'] = mystat['Sort_scan'] + mystat['Sort_range']
    mycalc['pct_temp_sort_table'] = int(mystat['Sort_merge_passes'] / mycalc['total_sorts'] * 100) if mycalc['total_sorts'] > 0 else 0

    # Joins
    mycalc['joins_without_indexes'] = mystat['Select_range_check'] + mystat['Select_full_join']
    mycalc['joins_without_indexes_per_day'] = int(mycalc['joins_without_indexes'] / (mystat['Uptime'] / 86400))

    # Temporary tables
    mycalc['pct_temp_disk'] = 0
    if mystat['Created_tmp_tables'] > 0:
        if mystat['Created_tmp_disk_tables'] > 0:
            mycalc['pct_temp_disk'] = int(mystat['Created_tmp_disk_tables'] / mystat['Created_tmp_tables'] * 100)

    # Table cache
    if mystat['Opened_tables'] > 0:
        mycalc['table_cache_hit_rate'] = int(mystat['Open_tables'] / mystat['Opened_tables'] * 100)
    else:
        mycalc['table_cache_hit_rate'] = 100

    # Open files
    mycalc['pct_files_open'] = int(mystat['Open_files'] / myvar['open_files_limit'] * 100) if myvar['open_files_limit'] > 0 else 0

    # Table locks
    if mystat['Table_locks_immediate'] > 0:
        if mystat['Table_locks_waited'] == 0:
            mycalc['pct_table_locks_immediate'] = 100
        else:
            mycalc['pct_table_locks_immediate'] = int(mystat['Table_locks_immediate'] / (mystat['Table_locks_waited'] + mystat['Table_locks_immediate']) * 100)

    # Thread cache
    mycalc['thread_cache_hit_rate'] = int(100 - mystat['Threads_created'] / mystat['Connections'] * 100)

    # Binlog Cache
    if myvar['log_bin'] != 'OFF':
        mycalc['pct_binlog_cache'] = int((mystat['Binlog_cache_use'] - mystat['Binlog_cache_disk_use']) / mystat['Binlog_cache_use'] * 100)




    msg = ''
    msg_warn = 'WARN: '
    perfdata = ''
    state = STATE_OK


    msg += '{} v{}.{}, '.format(mysqltype, mysqlvermajor, mysqlverminor)
    msg += '{}%/{}% r/w, '.format(mycalc['pct_reads'], mycalc['pct_writes'])
    msg += 'up {}, '.format(lib.base2.seconds2human(mystat['Uptime']))
    perfdata += lib.base2.get_perfdata('uptime', mystat['Uptime'], 's', None, None, 0, None)

    qps = round(mystat['Questions'] / mystat['Uptime'], 1)
    msg += '{} Questions, {} QPS, '.format(lib.base2.number2human(mystat['Questions']), qps)
    perfdata += lib.base2.get_perfdata('questions', mystat['Questions'], 'c', None, None, 0, None)
    perfdata += lib.base2.get_perfdata('qps', qps, 'c', None, None, 0, None)

    msg += '{} Connections, '.format(lib.base2.number2human(mystat['Connections']))
    perfdata += lib.base2.get_perfdata('connections', mystat['Connections'], 'c', None, None, 0, None)

    msg += '{}/{} (Rx/Tx)\n'.format(lib.base2.bytes2human(mystat['Bytes_received']), lib.base2.bytes2human(mystat['Bytes_sent']))
    perfdata += lib.base2.get_perfdata('bytes_received', mystat['Bytes_received'], 'B', None, None, 0, None)
    perfdata += lib.base2.get_perfdata('bytes_sent', mystat['Bytes_sent'], 'B', None, None, 0, None)


    msg += log_file_recommendations(myvar)

    # checks as done in mysqltuner
    # mainly from `sub mysql_stats`

    # Memory usage
    if mycalc['pct_max_physical_memory'] > 90:
        msg_warn += '* MySQL\'s maximum memory usage is dangerously high. Add RAM before increasing MySQL buffer variables.\n'
        state = lib.base2.get_worst(state, STATE_CRIT)
    elif mycalc['pct_max_physical_memory'] > 85:
        msg_warn += '* Maximum possible memory usage > 85% (reduce your overall MySQL memory footprint or increase RAM >= 2GB for system stability)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Maximum possible memory usage: {} ({}% of installed RAM)\n'.format(lib.base2.bytes2human(mycalc['max_peak_memory']), mycalc['pct_max_physical_memory'])
    perfdata += lib.base2.get_perfdata('pct_max_physical_memory', mycalc['pct_max_physical_memory'], '%', 85, None, 0, None)

    if mycalc['pct_max_used_memory'] > 85:
        msg_warn += '* Maximum reached memory usage > 85%\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Maximum reached memory usage: {} ({}% of installed RAM)\n'.format(lib.base2.bytes2human(mycalc['max_used_memory']), mycalc['pct_max_used_memory'])
    perfdata += lib.base2.get_perfdata('pct_max_used_memory', mycalc['pct_max_used_memory'], '%', 85, None, 0, 100)

    if physical_memory < mycalc['max_peak_memory'] + get_other_process_memory():
        msg_warn += '* Overall possible memory usage with other process exceeded memory (dedicate this server to your database for highest performance)\n'
        state = lib.base2.get_worst(state, STATE_WARN)

    # Slow queries
    if mycalc['pct_slow_queries'] > 5:
        msg_warn += '* Slow Queries > 5% (use the slow query log to troubleshoot bad queries)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Slow queries: {}% ({}/{})\n'.format(mycalc['pct_slow_queries'], mystat['Slow_queries'], lib.base2.number2human(mystat['Questions']))
    perfdata += lib.base2.get_perfdata('pct_slow_queries', mycalc['pct_slow_queries'], '%', 5, None, 0, 100)

    # Connections
    if mycalc['pct_connections_used'] > 85:
        msg_warn += '* Connections used > 85% (reduce or eliminate persistent connections to reduce connection usage, increase max_connections, decrease wait_timeout and/or decrease interactive_timeout)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Highest connection usage: {}% ({}/{})\n'.format(mycalc['pct_connections_used'], mystat['Max_used_connections'], myvar['max_connections'])
    perfdata += lib.base2.get_perfdata('pct_connections_used', mycalc['pct_connections_used'], '%', 85, None, 0, 100)

    # Aborted Connections
    if mycalc['pct_connections_aborted'] > 3:
        msg_warn += '* Connections aborted > 3% (reduce or eliminate unclosed connections and network issues)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Aborted connections: {}% ({}/{})\n'.format(mycalc['pct_connections_aborted'], mystat['Aborted_connects'], mystat['Connections'])
    perfdata += lib.base2.get_perfdata('pct_connections_aborted', mycalc['pct_connections_aborted'], '%', 85, None, 0, 100)

    # name resolution
    if 'skip_networking' in myvar and myvar['skip_networking'] == 'OFF':
        if 'skip_name_resolve' in myvar and myvar['skip_name_resolve'] == 'OFF':
            msg_warn += '* Name resolution is active: a reverse name resolution is made for each new connection and can reduce performance (configure your accounts with ip or subnets only, set skip_name_resolve=1)\n'
            state = lib.base2.get_worst(state, STATE_WARN)

    # Query cache
    if mysqlvermajor < 4 or (mysqlvermajor >= 8 and mysqltype == 'MySQL'):
        # MySQL versions < 4.01 don't support query caching
        # Query cache have been removed in MySQL 8"
        tmp = 'do nothing'
    elif myvar['query_cache_size'] < 1 and myvar['query_cache_type'] == 'OFF':
        # Query cache is disabled by default due to mutex contention on multiprocessor machines.
        tmp = 'do nothing'
    elif mystat['Com_select'] == 0:
        msg_warn += '* Query cache cannot be analyzed - no SELECT statements executed\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    else:
        msg_warn += '* Query cache should be disabled due to mutex contention on multiprocessor machines (set query_cache_size=0, query_cache_type=OFF)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
        if mycalc['query_cache_efficiency'] < 20:
            msg_warn += '* Query cache efficiency < 20% (increase query_cache_limit or use smaller result sets)\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        msg += '* Query cache efficiency: {}% ({} cached / {} selects)\n'.format(mycalc['query_cache_efficiency'], mystat['Qcache_hits'], lib.base2.number2human(mystat['Qcache_hits'] + mystat['Com_select']))
        perfdata += lib.base2.get_perfdata('query_cache_efficiency', mycalc['query_cache_efficiency'], '%', None, None, 0, 100)

        if mycalc['query_cache_prunes_per_day'] > 98:
            if myvar['query_cache_size'] >= 128 * 1024 * 1024:
                msg_warn += '* Increasing the query_cache size over 128M may reduce performance\n'
            else:
                msg_warn += '* Increase query_cache_size up to 128M\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        msg += '* Query cache prunes per day: {}\n'.format(mycalc['query_cache_prunes_per_day'])
        perfdata += lib.base2.get_perfdata('query_cache_prunes_per_day', mycalc['query_cache_prunes_per_day'], None, 98, None, 0, None)

    # Sorting
    if mycalc['total_sorts'] == 0:
        # No Sort requiring temporary tables
        tmp = 'do nothing'
    else:
        if mycalc['pct_temp_sort_table'] > 10:
            msg_warn += '* Sorts are requiring temporary tables (increase sort_buffer_size, increase read_rnd_buffer_size)\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        msg += '* Sorts requiring temporary tables: {}% ({} temp sorts / {} sorts)\n'.format(mycalc['pct_temp_sort_table'], mystat['Sort_merge_passes'], lib.base2.number2human(mycalc['total_sorts']))
        perfdata += lib.base2.get_perfdata('pct_temp_sort_table', mycalc['pct_temp_sort_table'], '%', 10, None, 0, 100)

    # Joins
    if mycalc['joins_without_indexes_per_day'] > 250:
        msg_warn += '* More than 250 JOINs without indexes per day (increase join_buffer_size until JOINs not using indexes are found, or always use indexes with JOINs)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Joins performed without indexes: {}\n'.format(mycalc['joins_without_indexes'])
    perfdata += lib.base2.get_perfdata('joins_without_indexes', mycalc['joins_without_indexes'], None, None, None, 0, None)

    # Temporary tables
    if mystat['Created_tmp_tables'] > 0:
        if mycalc['pct_temp_disk'] > 25 and mycalc['max_tmp_table_size'] < 256 * 1024 * 1024:
            msg_warn += '* Reduce your SELECT DISTINCT queries without LIMIT clause, and/or increase tmp_table_size + increase max_heap_table_size (when making adjustments, make tmp_table_size/max_heap_table_size equal)\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        elif mycalc['pct_temp_disk'] > 25 and mycalc['max_tmp_table_size'] >= 256 * 1024 * 1024:
            msg_warn += '* Reduce your SELECT DISTINCT queries without LIMIT clause, and reduce result set size (temporary table size is already large)\n'
            state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Temporary tables created on disk: {}% ({} on disk / {} total)\n'.format(mycalc['pct_temp_disk'], lib.base2.number2human(mystat['Created_tmp_disk_tables']), lib.base2.number2human(mystat['Created_tmp_tables']))
    perfdata += lib.base2.get_perfdata('pct_temp_disk', mycalc['pct_temp_disk'], '%', None, None, 0, 100)

    # Thread cache
    if 'thread_handling' in myvar and myvar['thread_handling'] == 'pool-of-threads':
        # https://www.percona.com/doc/percona-server/LATEST/performance/threadpool.html
        # When thread pool is enabled, the value of the thread_cache_size variable
        # is ignored. The Threads_cached status variable contains 0 in this case.
        tmp = 'do nothing'
    else:
        if myvar['thread_cache_size'] == 0:
            msg_warn += '* Thread cache is disabled (set thread_cache_size, start at 4)\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        else:
            if mycalc['thread_cache_hit_rate'] <= 50:
                msg_warn += '* Thread cache hit rate <= 50% (increase thread_cache_size)\n'
                state = lib.base2.get_worst(state, STATE_WARN)
            msg += '* Thread cache hit rate: {}% ({} created / {} connections)\n'.format(mycalc['thread_cache_hit_rate'], mystat['Threads_created'], mystat['Connections'])
            perfdata += lib.base2.get_perfdata('thread_cache_hit_rate', mycalc['thread_cache_hit_rate'], '%', 50, None, 0, 100)

    # Table cache
    table_cache_var = ''
    if mystat['Open_tables'] > 0:
        if mycalc['table_cache_hit_rate'] < 20:
            if mysqlvermajor >= 5 and mysqlverminor >= 1:
                table_cache_var = 'table_open_cache'
            else:
                table_cache_var = 'table_cache'
            msg_warn += '* Table cache hit rate < 20% (increase {} gradually to avoid file descriptor limits, read https://mariadb.com/kb/en/library/optimizing-table_open_cache/ before increasing for MariaDB)\n'.format(table_cache_var)
            state = lib.base2.get_worst(state, STATE_WARN)
        msg += '* Table cache hit rate: {}% ({} open / {} opened)\n'.format(mycalc['table_cache_hit_rate'], mystat['Open_tables'], mystat['Opened_tables'])
        perfdata += lib.base2.get_perfdata('table_cache_hit_rate', mycalc['table_cache_hit_rate'], '%', None, None, 0, 100)

    # Table definition cache
    if 'table_definition_cache' in myvar:
        if myvar['table_definition_cache'] == -1:
            tmp = 'do nothing'
        elif myvar['table_definition_cache'] < nbtables[0]:
            msg_warn += '* table_definition_cache is lower than number of tables (increase table_definition_cache or enable autosizing by setting table_definition_cache=-1)\n'
            state = lib.base2.get_worst(state, STATE_WARN)

    # Open files
    if 'pct_files_open' in mycalc and mycalc['pct_files_open'] > 85:
        msg_warn += '* Open files > 85% (increase open_files_limit)\n'
        state = lib.base2.get_worst(state, STATE_WARN)
    msg += '* Open file limit used: {}% ({}/{})\n'.format(mycalc['pct_files_open'], mystat['Open_files'], myvar['open_files_limit'])
    perfdata += lib.base2.get_perfdata('pct_files_open', mycalc['pct_files_open'], '%', 85, None, 0, 100)

    # Table locks
    if 'pct_table_locks_immediate' in mycalc:
        if mycalc['pct_table_locks_immediate'] < 95:
            msg_warn += '* Table locks acquired immediately < 95% (optimize queries and/or use InnoDB to reduce lock wait)\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        msg += '* Table locks acquired immediately: {}% ({} immediate / {} locks)\n'.format(mycalc['pct_table_locks_immediate'], mystat['Table_locks_immediate'], mystat['Table_locks_waited'] + mystat['Table_locks_immediate'])
        perfdata += lib.base2.get_perfdata('pct_table_locks_immediate', mycalc['pct_table_locks_immediate'], '%', None, None, 0, 100)

    # Binlog cache
    if 'pct_binlog_cache' in mycalc:
        if mycalc['pct_binlog_cache'] < 90 and mystat['Binlog_cache_use'] > 0:
            msg_warn += '* Increase binlog_cache_size\n'
            state = lib.base2.get_worst(state, STATE_WARN)
        msg += '* Binlog cache memory access: {}% ({} Memory / {} Total)\n'.format(mycalc['pct_binlog_cache'], mystat['Binlog_cache_use'] - mystat['Binlog_cache_disk_use'], mystat['Binlog_cache_use'])
        perfdata += lib.base2.get_perfdata('pct_binlog_cache', mycalc['pct_binlog_cache'], '%', None, None, 0, 100)

    # Performance options
    if 'concurrent_insert' in myvar and (myvar['concurrent_insert'] == 0 or myvar['concurrent_insert'] == 'OFF'):
        msg_warn += '* Enable concurrent_insert\n'
        state = lib.base2.get_worst(state, STATE_WARN)


    if msg_warn != 'WARN: ':
        msg = msg_warn.replace('WARN: ', 'There are warnings.\n').strip() + '\n---\n' + msg


    lib.base2.oao(msg, state, perfdata, always_ok=args.ALWAYS_OK)


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print_exc()
        exit(STATE_UNKNOWN)
